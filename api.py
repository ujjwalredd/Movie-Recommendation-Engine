#!/usr/bin/env python3
"""
Movie Recommendation Engine REST API

FastAPI-based REST API for movie recommendations with comprehensive endpoints               .
"""

from fastapi import FastAPI, HTTPException, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import uvicorn
import time
import os
import sys
import json
import numpy as np

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data.data_loader import MovieLensDataLoader
from models import (
    MatrixFactorization, NeuralCollaborativeFiltering, HybridRecommendationModel,
    UserBasedCF, ItemBasedCF
)
from evaluation.evaluator import ModelEvaluator
from utils.trainer import ModelTrainer
from utils.inference_optimizer import ModelInferenceManager


# Initialize FastAPI app
app = FastAPI(
    title="Movie Recommendation Engine API",
    description="A comprehensive REST API for movie recommendations using collaborative filtering and deep learning",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables for data and models
data_loader = None
models = {}
inference_manager = ModelInferenceManager()
evaluator = ModelEvaluator()


# Pydantic models for request/response
class RecommendationRequest(BaseModel):
    user_id: int = Field(..., description="User ID to get recommendations for")
    n_recommendations: int = Field(10, ge=1, le=50, description="Number of recommendations to return")
    model_type: Optional[str] = Field("hybrid", description="Type of model to use")

class RecommendationResponse(BaseModel):
    user_id: int
    recommendations: List[Dict[str, Any]]
    model_type: str
    latency_ms: float
    timestamp: str

class RatingRequest(BaseModel):
    user_id: int = Field(..., description="User ID")
    movie_id: int = Field(..., description="Movie ID")
    rating: float = Field(..., ge=1.0, le=5.0, description="Rating value (1-5)")

class RatingResponse(BaseModel):
    user_id: int
    movie_id: int
    rating: float
    predicted_rating: Optional[float] = None
    timestamp: str

class ModelInfo(BaseModel):
    model_type: str
    num_users: int
    num_items: int
    num_parameters: int
    model_size_mb: float
    is_loaded: bool

class PerformanceMetrics(BaseModel):
    model_type: str
    precision_at_10: float
    recall_at_10: float
    ndcg_at_10: float
    rmse: float
    mae: float
    mean_latency_ms: float

class ABTestRequest(BaseModel):
    experiment_name: str
    variants: List[str]
    traffic_split: Dict[str, float]
    duration_days: int = 30

class ABTestResponse(BaseModel):
    experiment_id: str
    status: str
    variants: List[str]
    traffic_split: Dict[str, float]
    start_time: str


# Dependency to load data and models
def get_data_loader():
    """Get or create data loader."""
    global data_loader
    if data_loader is None:
        try:
            data_loader = MovieLensDataLoader("data/movielens", "1m")
            data_loader.load_data()
            data_loader.create_mappings()
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load data: {str(e)}")
    return data_loader


def get_models():
    """Get or create models."""
    global models, inference_manager
    if not models:
        try:
            dl = get_data_loader()
            
            # Create models
            models['matrix_factorization'] = MatrixFactorization(
                num_users=dl.num_users,
                num_items=dl.num_items,
                num_factors=50
            )
            
            models['neural_cf'] = NeuralCollaborativeFiltering(
                num_users=dl.num_users,
                num_items=dl.num_items,
                embedding_dim=50
            )
            
            models['hybrid'] = HybridRecommendationModel(
                num_users=dl.num_users,
                num_items=dl.num_items,
                mf_factors=50,
                neural_layers=[100, 50, 20],
                embedding_dim=50
            )
            
            # Load pre-trained weights if available
            trainer = ModelTrainer()
            model_paths = {
                'matrix_factorization': 'best_mf_model.pth',
                'neural_cf': 'best_neural_cf_model.pth',
                'hybrid': 'best_hybrid_model.pth'
            }
            
            for model_name, model in models.items():
                model_path = model_paths.get(model_name)
                if model_path and os.path.exists(model_path):
                    try:
                        trainer.load_model(model, model_path)
                        print(f"âœ… Loaded {model_name} model from {model_path}")
                        # Add to inference manager for optimization
                        inference_manager.add_model(model_name, model, optimize=True)
                    except Exception as e:
                        print(f"âš ï¸ Warning: Could not load {model_name} model: {e}")
                else:
                    print(f"â„¹ï¸ No trained model found for {model_name}")
            
            # Precompute embeddings for faster inference
            print("ðŸš€ Precomputing embeddings for optimized inference...")
            inference_manager.precompute_all_embeddings()
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load models: {str(e)}")
    return models


# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "version": "1.0.0"
    }


# Root endpoint
@app.get("/")
async def root():
    """Root endpoint with API information."""
    return {
        "message": "Movie Recommendation Engine API",
        "version": "1.0.0",
        "docs": "/docs",
        "endpoints": {
            "health": "/health",
            "recommendations": "/recommendations",
            "rate_movie": "/rate",
            "models": "/models",
            "performance": "/performance",
            "ab_test": "/ab-test"
        }
    }


# Get recommendations endpoint
@app.post("/recommendations", response_model=RecommendationResponse)
async def get_recommendations(
    request: RecommendationRequest,
    dl: MovieLensDataLoader = Depends(get_data_loader),
    model_dict: Dict = Depends(get_models)
):
    
    try:
        # Validate user ID
        if request.user_id not in dl.user_id_map:
            raise HTTPException(status_code=404, detail=f"User {request.user_id} not found")
        
        # Get model
        if request.model_type not in model_dict:
            raise HTTPException(status_code=400, detail=f"Model type '{request.model_type}' not available")
        
        model = model_dict[request.model_type]
        
        # Get user index
        user_idx = dl.user_id_map[request.user_id]
        
        # Get recommendations with optimized inference
        start_time = time.time()
        recommendations = inference_manager.get_recommendations(request.model_type, user_idx, request.n_recommendations)
        latency = (time.time() - start_time) * 1000
        
        # Format recommendations
        formatted_recommendations = []
        for i, movie_idx in enumerate(recommendations):
            # Convert numpy.int64 to regular int for serialization
            movie_idx = int(movie_idx)
            original_movie_id = int(dl.reverse_item_map[movie_idx])
            
            # Get movie info if available
            movie_info = {
                "movie_id": original_movie_id,
                "rank": i + 1,
                "title": f"Movie {original_movie_id}"  # Placeholder
            }
            
            formatted_recommendations.append(movie_info)
        
        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=formatted_recommendations,
            model_type=request.model_type,
            latency_ms=latency,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting recommendations: {str(e)}")


# Rate movie endpoint
@app.post("/rate", response_model=RatingResponse)
async def rate_movie(
    request: RatingRequest,
    dl: MovieLensDataLoader = Depends(get_data_loader),
    model_dict: Dict = Depends(get_models)
):
    """Rate a movie and optionally get prediction."""
    
    try:
        # Validate user and movie IDs
        if request.user_id not in dl.user_id_map:
            raise HTTPException(status_code=404, detail=f"User {request.user_id} not found")
        
        if request.movie_id not in dl.item_id_map:
            raise HTTPException(status_code=404, detail=f"Movie {request.movie_id} not found")
        
        # Get prediction from hybrid model if available
        predicted_rating = None
        if 'hybrid' in model_dict:
            try:
                user_idx = dl.user_id_map[request.user_id]
                movie_idx = dl.item_id_map[request.movie_id]
                predicted_rating = model_dict['hybrid'].predict(user_idx, movie_idx)
            except Exception as e:
                print(f"Warning: Could not get prediction: {e}")
        
        return RatingResponse(
            user_id=request.user_id,
            movie_id=request.movie_id,
            rating=request.rating,
            predicted_rating=predicted_rating,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error rating movie: {str(e)}")


# Get available models endpoint
@app.get("/models", response_model=List[ModelInfo])
async def get_models_info(
    dl: MovieLensDataLoader = Depends(get_data_loader),
    model_dict: Dict = Depends(get_models)
):
    """Get information about available models."""
    
    models_info = []
    
    for model_name, model in model_dict.items():
        # Get model summary
        trainer = ModelTrainer()
        summary = trainer.get_model_summary(model)
        
        models_info.append(ModelInfo(
            model_type=model_name,
            num_users=int(dl.num_users),
            num_items=int(dl.num_items),
            num_parameters=int(summary['total_parameters']),
            model_size_mb=float(summary['model_size_mb']),
            is_loaded=True
        ))
    
    return models_info


# Get model performance endpoint
@app.get("/performance", response_model=List[PerformanceMetrics])
async def get_model_performance(
    dl: MovieLensDataLoader = Depends(get_data_loader),
    model_dict: Dict = Depends(get_models)
):
    """Get performance metrics for all models."""
    
    # This would typically load test data and evaluate models
    # For now, return simulated metrics
    
    performance_data = []
    
    for model_name in model_dict.keys():
        # Get real inference performance if available
        inference_stats = inference_manager.get_model_performance(model_name)
        
        if inference_stats.get('status') == 'not_optimized':
            mean_latency = float(150 + np.random.normal(0, 20))
        else:
            mean_latency = float(inference_stats.get('avg_latency_ms', 150))
        
        # Simulate other performance metrics
        metrics = PerformanceMetrics(
            model_type=model_name,
            precision_at_10=float(0.85 + np.random.normal(0, 0.02)),
            recall_at_10=float(0.48 + np.random.normal(0, 0.03)),
            ndcg_at_10=float(0.81 + np.random.normal(0, 0.02)),
            rmse=float(0.85 + np.random.normal(0, 0.05)),
            mae=float(0.68 + np.random.normal(0, 0.03)),
            mean_latency_ms=mean_latency
        )
        performance_data.append(metrics)
    
    return performance_data


# Get inference performance statistics endpoint
@app.get("/inference/stats")
async def get_inference_stats():
    """Get detailed inference performance statistics."""
    stats = {}
    
    for model_name in inference_manager.models.keys():
        model_stats = inference_manager.get_model_performance(model_name)
        if model_stats.get('status') != 'not_optimized':
            stats[model_name] = model_stats
    
    return {
        "inference_stats": stats,
        "total_models": len(inference_manager.models),
        "optimized_models": len([s for s in stats.values() if s.get('status') != 'not_optimized'])
    }


# A/B testing endpoints
@app.post("/ab-test", response_model=ABTestResponse)
async def create_ab_test(request: ABTestRequest):
    """Create a new A/B test experiment."""
    
    try:
        # Validate traffic split
        total_traffic = sum(request.traffic_split.values())
        if abs(total_traffic - 1.0) > 1e-6:
            raise HTTPException(status_code=400, detail="Traffic split must sum to 1.0")
        
        # Generate experiment ID
        experiment_id = f"exp_{int(time.time())}"
        
        return ABTestResponse(
            experiment_id=experiment_id,
            status="active",
            variants=request.variants,
            traffic_split=request.traffic_split,
            start_time=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating A/B test: {str(e)}")


@app.get("/ab-test/{experiment_id}")
async def get_ab_test_results(experiment_id: str):
    """Get A/B test results."""
    
    # Simulate A/B test results
    results = {
        "experiment_id": experiment_id,
        "status": "active",
        "duration_days": 14,
        "total_users": 1000,
        "total_interactions": 5000,
        "variants": {
            "matrix_factorization": {
                "users": 250,
                "clicks": 120,
                "avg_rating": 3.8,
                "click_through_rate": 0.48
            },
            "neural_cf": {
                "users": 250,
                "clicks": 135,
                "avg_rating": 4.1,
                "click_through_rate": 0.54
            },
            "hybrid": {
                "users": 500,
                "clicks": 300,
                "avg_rating": 4.3,
                "click_through_rate": 0.60
            }
        },
        "statistical_significance": {
            "hybrid_vs_mf": {"p_value": 0.001, "significant": True},
            "hybrid_vs_neural": {"p_value": 0.045, "significant": True}
        }
    }
    
    return results


# Batch recommendations endpoint
@app.post("/recommendations/batch")
async def get_batch_recommendations(
    user_ids: List[int] = Query(..., description="List of user IDs"),
    n_recommendations: int = Query(10, ge=1, le=50),
    model_type: str = Query("hybrid", description="Type of model to use"),
    dl: MovieLensDataLoader = Depends(get_data_loader),
    model_dict: Dict = Depends(get_models)
):
    """Get recommendations for multiple users in batch."""
    
    try:
        if model_type not in model_dict:
            raise HTTPException(status_code=400, detail=f"Model type '{model_type}' not available")
        
        model = model_dict[model_type]
        results = []
        
        start_time = time.time()
        
        for user_id in user_ids:
            if user_id not in dl.user_id_map:
                continue
            
            user_idx = dl.user_id_map[user_id]
            recommendations = model.get_user_recommendations(user_idx, n_recommendations)
            
            # Format recommendations
            formatted_recs = []
            for i, movie_idx in enumerate(recommendations):
                original_movie_id = dl.reverse_item_map[movie_idx]
                formatted_recs.append({
                    "movie_id": original_movie_id,
                    "rank": i + 1
                })
            
            results.append({
                "user_id": user_id,
                "recommendations": formatted_recs
            })
        
        total_time = time.time() - start_time
        
        return {
            "results": results,
            "total_users": len(results),
            "total_time_seconds": total_time,
            "avg_time_per_user_ms": (total_time / len(results)) * 1000 if results else 0,
            "model_type": model_type
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting batch recommendations: {str(e)}")


# Model training endpoint
@app.post("/train")
async def train_model(
    model_type: str = Query(..., description="Type of model to train"),
    epochs: int = Query(100, ge=1, le=1000),
    learning_rate: float = Query(0.001, ge=0.0001, le=0.1),
    batch_size: int = Query(1024, ge=64, le=4096),
    dl: MovieLensDataLoader = Depends(get_data_loader)
):
    """Train a recommendation model."""
    
    try:
        # This would typically start training in background
        # For now, return training configuration
        
        training_config = {
            "model_type": model_type,
            "epochs": epochs,
            "learning_rate": learning_rate,
            "batch_size": batch_size,
            "num_users": dl.num_users,
            "num_items": dl.num_items,
            "status": "scheduled",
            "estimated_time_minutes": epochs * 2,  # Rough estimate
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return training_config
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error scheduling training: {str(e)}")


# Dataset statistics endpoint
@app.get("/dataset/stats")
async def get_dataset_stats(dl: MovieLensDataLoader = Depends(get_data_loader)):
    """Get dataset statistics."""
    
    try:
        stats = {
            "num_users": dl.num_users,
            "num_items": dl.num_items,
            "num_ratings": len(dl.ratings_df) if dl.ratings_df is not None else 0,
            "sparsity": 1 - (len(dl.ratings_df) / (dl.num_users * dl.num_items)) if dl.ratings_df is not None else 0,
            "avg_rating": dl.ratings_df['rating'].mean() if dl.ratings_df is not None else 0,
            "rating_distribution": dl.ratings_df['rating'].value_counts().to_dict() if dl.ratings_df is not None else {}
        }
        
        return stats
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting dataset stats: {str(e)}")


# Search movies endpoint
@app.get("/movies/search")
async def search_movies(
    query: str = Query(..., description="Search query"),
    limit: int = Query(10, ge=1, le=100),
    dl: MovieLensDataLoader = Depends(get_data_loader)
):
    """Search for movies by title."""
    
    try:
        if dl.movies_df is None:
            raise HTTPException(status_code=500, detail="Movie data not available")
        
        # Simple search implementation
        movies = dl.movies_df[dl.movies_df['title'].str.contains(query, case=False, na=False)]
        movies = movies.head(limit)
        
        results = []
        for _, movie in movies.iterrows():
            results.append({
                "movie_id": movie['movieId'],
                "title": movie['title'],
                "genres": movie.get('genres', 'Unknown')
            })
        
        return {
            "query": query,
            "results": results,
            "total_found": len(results)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error searching movies: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    ) 